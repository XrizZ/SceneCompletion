<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Scene Completion Using Millions of Photographs &ndash; Implementation</title>
<link rel="stylesheet" type="text/css" href="style.css" />
</head>
<body>

<div id="pagebox">
	<h1>
		<img src="img/logo_tu.png" class="left_side" />
		<img src="img/logo_cg.png" class="right_side" />
		Scene Completion Using Millions of Photographs &ndash; An Implementation
	</h1>
	<div id="menu">
		<a href="index.html">Introduction</a>
		<a href="implementation.html">Implementation</a>
		<a href="results.html">Results</a>
		<a href="contact.html">Contact</a>
	</div>

	<h2>Implementation</h2>
	<p>
		Our program was coded in C++ and extensively uses OpenCV to do image processing.
		To be able to run a whole set of images in a row, we added a batch script that doesn't rely on user interaction.
	</p>
	
	<h3>GIST</h3>
	<p>
		<div class="imagebox right_side">
			<img src="img/gist.png" />
			Figure 1: 6x6 tile Layout of GIST Descriptor
		</div>
		Searching a database of millions of images is quite an effort. On the one hand, hundreds of gigabytes need to be read
		which already takes a lot of time, depending on the medium they are saved on. Also comparing images to one another
		is time-consuming. On the other hand, when looking for images, we are not interested in single pixels being similar or different.
		This is the reason we use GIST to describe images for us.<br/>
		Every image's GIST descriptor is pre-calculated. This means that the image is divided into tiles and for each tile
		edges of different frequencies and angular orientations are being detected and represented by a number.
		When comparing two images, those numbers can be compared component-by-component and
		afterwards summed up to give us a single value describing the similarity between two images. For the images in our database,
		the GIST descriptors are not going to change over time. This is why it is reasonable to pre-calculate them to be able to just read them
		again when they are needed.
		Our GIST algorithm uses only greyscale information about the images. It doesn't care about colors.
	</p>
	<p>
		Building the big database took about 49 hours on a modern CPU with 4 cores.
		However, browsing the database for similarities with an input image takes only about 7 minutes.
	</p>
	
	<h3>Best Fit</h3>
	<p>
		The original algorithm, proposed in the paper, suggests to calculate the best fit in the L*a*b color space by comparing rotated, 
		scaled and translated versions of similar image to the query image. Although it may be a simple task, this is very computationally expensive.
		Depending on the dimensionality of the problem, this would add a considerably large amount to the overall processing time. We found that despite
		not having implemented this feature, the results are comparable to the original implementation of the papers algorithm.
	</p>
	
	<h3>Graph Cut</h3>
	<p>
		<div class="imagebox left_side">
			<img src="img/graphcut.png" />
			Figure 2: left: Input Mask; right: Refined Mask after Graph Cut
		</div>
		When a candidate for blending has been found, we improve the input mask using a graph cut algorithm.
		The input mask already covers the areas of the input image that the user specified. But the edges of the mask
		are not necessarily the best seams to stitch the images. In case the gradients of both images are very different to each other,
		two very semantically different areas may be stitched together. In order to reduce that risk, we allow the mask to leave from its
		original path and find its place some pixel lengths further if that helps to make the subsequent blending appear more plausible.<br/>
		The graph we are building gets one node for each pixel that will be in the final image.
		Each pixel node is connected by edges to its four immediate neighbors.
		The weights of these edges later decide where the best cut will be.
		This is where we put the difference of both images' gradients combined with a cost function that assumes very low values on the original
		path of the mask and quickly rises for growing distance.
		To create a flow inside the graph, we also need sources and sinks for it. Those are placed inside the mask
		and on the borders of the input image so that the flow definitely goes through the border of the original mask. We experimented with the weighting
		function and found that an extension to the original implementation may be helpful in some cases, where we allow not only adding pixels to the mask,
		but also cutting out pixels from the mask if it helps minimizing the cutting costs. For calculating the minimal cut, we utilized a library 
		implementing the <a href="http://www.csd.uwo.ca/~yuri/Papers/pami04.pdf">max-flow algorithm</a>.
	</p>
	<p>
		Calculating the best cut takes about 1 minute for 15 results.
	</p>
	
	<h3>Poisson Blending</h3>
	<p>
		In order to get convincing results, we use Poisson blending for the actual joining of the images. From our experience, once a similar image is found,
		this step adds the most improvement to the final result.
		To solve the Poisson equations we make use of the UMFPACK library. For that we adapted the implementation found
		<a href="http://opencv.jp/opencv2-x-samples/poisson-blending">here</a> and made several improvements to it.
	</p>
	<p>
		The runtime of the Poisson blending mainly depends on the size of the final mask. 15 images are usually blended within 2 to 4 minutes.
	</p>
	
	<p>
		For an evaluation of the results, please refer to the <a href="results.html">next</a> section.
	</p>

</div>

</body>
</html>